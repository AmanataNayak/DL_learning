{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dim: int):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias) of type float\n",
    "    \"\"\"\n",
    "    w: np.array = np.zeros((dim, 1))\n",
    "    b: float = float(0)\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.array):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1 / (1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w: np.array, b: float, X: np.array, Y: np.array):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    grads -- dictionary containing the gradients of the weights and bias\n",
    "            (dw -- gradient of the loss with respect to w, thus same shape as w)\n",
    "            (db -- gradient of the loss with respect to b, thus same shape as b)\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X)+b)\n",
    "    cost = -1/m * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
    "    dz = A - Y\n",
    "    dw = 1/m * np.dot(X, dz.T)\n",
    "    db = 1/m * np.sum(dz)\n",
    "\n",
    "    grads = {\n",
    "        'dw': dw,\n",
    "        'db': db\n",
    "    }\n",
    "\n",
    "    return grads, cost\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w: np.array, b: float, X: np.array, Y, num_iterations: int=100, learning_rate: float=0.009, print_cost: bool=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b.\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(f'Cost after iteration {i}: {cost}')\n",
    "\n",
    "    params = {\n",
    "        'w': w,\n",
    "        'b': b\n",
    "    }\n",
    "\n",
    "\n",
    "    return params, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w: np.array, b: float, X: np.array):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0, i] > 0.5:\n",
    "            Y_prediction[0, i] = 1\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train: np.array, Y_train: np.array, X_test: np.array, Y_test: np.array, num_iterations: int=2000, learning_rate: int=0.5, print_cost: bool=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to True to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "\n",
    "    w, b = initialize(X_train.shape[0])\n",
    "\n",
    "    params, costs = optimize(w, b , X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    if print_cost:\n",
    "        print(f'training accuracy: {100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100}')\n",
    "        print(f'testing accuracy: {100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100}')\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.0397207708399179\n",
      "Cost after iteration 100: 0.9548641184480434\n",
      "Cost after iteration 200: 0.9548349070782658\n",
      "Cost after iteration 300: 0.954814885430295\n",
      "Cost after iteration 400: 0.954801162016137\n",
      "Cost after iteration 500: 0.9547917553075961\n",
      "Cost after iteration 600: 0.9547853073089909\n",
      "Cost after iteration 700: 0.9547808873070643\n",
      "Cost after iteration 800: 0.9547778574029899\n",
      "Cost after iteration 900: 0.9547757803722148\n",
      "Cost after iteration 1000: 0.9547743565246888\n",
      "Cost after iteration 1100: 0.9547733804354022\n",
      "Cost after iteration 1200: 0.9547727112903779\n",
      "Cost after iteration 1300: 0.9547722525627841\n",
      "Cost after iteration 1400: 0.9547719380829871\n",
      "Cost after iteration 1500: 0.9547717224906233\n",
      "Cost after iteration 1600: 0.9547715746899936\n",
      "Cost after iteration 1700: 0.9547714733639467\n",
      "Cost after iteration 1800: 0.9547714038987128\n",
      "Cost after iteration 1900: 0.9547713562758802\n",
      "training accuracy: 66.66666666666667\n",
      "testing accuracy: 50.0\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[0.1, 0.2, 0.3], [0.3, 0.4, 0.5]])\n",
    "Y_train = np.array([[0, 1, 0]])\n",
    "X_test = np.array([[0.2, 0.4], [0.5, 0.6]])\n",
    "Y_test = np.array([[0, 1]])\n",
    "\n",
    "d = model(X_train, Y_train, X_test, Y_test, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
